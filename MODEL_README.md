---
language:
- vi
license: mit
tags:
- summarization
- vietnamese
- news
- vit5
- abstractive-summarization
datasets:
- 8Opt/vietnamese-summarization-dataset-0001
metrics:
- rouge
pipeline_tag: summarization
widget:
- text: "ChÃ­nh phá»§ Viá»‡t Nam Ä‘Ã£ ban hÃ nh quy Ä‘á»‹nh má»›i vá» thuáº¿ thu nháº­p cÃ¡ nhÃ¢n, Ã¡p dá»¥ng tá»« ngÃ y 1 thÃ¡ng 1 nÄƒm 2026. Theo Ä‘Ã³, má»©c giáº£m trá»« gia cáº£nh sáº½ Ä‘Æ°á»£c tÄƒng tá»« 11 triá»‡u Ä‘á»“ng lÃªn 13 triá»‡u Ä‘á»“ng má»—i thÃ¡ng. ÄÃ¢y lÃ  má»©c tÄƒng cao nháº¥t trong 5 nÄƒm qua, nháº±m giáº£m gÃ¡nh náº·ng thuáº¿ cho ngÆ°á»i lao Ä‘á»™ng vÃ  kÃ­ch thÃ­ch tiÃªu dÃ¹ng. Bá»™ TÃ i chÃ­nh cho biáº¿t chÃ­nh sÃ¡ch nÃ y sáº½ cÃ³ tÃ¡c Ä‘á»™ng tÃ­ch cá»±c Ä‘áº¿n khoáº£ng 15 triá»‡u ngÆ°á»i Ä‘Ã³ng thuáº¿ thu nháº­p cÃ¡ nhÃ¢n trÃªn toÃ n quá»‘c."
  example_title: "Tin tá»©c chÃ­nh trá»‹"
---

# ViT5 Vietnamese News Summarization (Abstractive)

MÃ´ hÃ¬nh tÃ³m táº¯t tin tá»©c tiáº¿ng Viá»‡t tá»± Ä‘á»™ng sá»­ dá»¥ng kiáº¿n trÃºc **ViT5-base** (Vietnamese T5), Ä‘Æ°á»£c fine-tune cho bÃ i toÃ¡n **abstractive summarization** (tÃ³m táº¯t trá»«u tÆ°á»£ng).

## ğŸ“Š Model Description

- **Base Model**: [VietAI/vit5-base](https://huggingface.co/VietAI/vit5-base)
- **Task**: Abstractive Text Summarization
- **Language**: Vietnamese (Tiáº¿ng Viá»‡t)
- **Parameters**: 220M
- **License**: MIT
- **Training Dataset**: [8Opt/vietnamese-summarization-dataset-0001](https://huggingface.co/datasets/8Opt/vietnamese-summarization-dataset-0001)

## ğŸ¯ Performance

Evaluated on 200 samples from VietNews test set:

| Metric | Score |
|--------|-------|
| **ROUGE-1** | 45.17% |
| **ROUGE-2** | 22.18% |
| **ROUGE-L** | 27.60% |
| **BERT F1** | 69.22% |
| **Inference Time** | 10.97s/sample (CPU) |
| **Compression Ratio** | 0.292 |

### Comparison with Extractive Model

| Model | ROUGE-1 | ROUGE-2 | ROUGE-L | BERT F1 | Speed |
|-------|---------|---------|---------|---------|-------|
| **Abstractive** (this) | 45.17% | 22.18% | 27.60% | 69.22% | **10.97s** âš¡ |
| Extractive | 50.48% | 22.84% | 30.61% | 71.33% | 20.46s |

**Highlights:**
- âœ… **2x faster** than extractive model
- âœ… **Shorter summaries** (63.5 words vs 96.3 words)
- âœ… **More natural** paraphrasing instead of copying sentences
- âš ï¸ Slightly lower ROUGE scores (expected for abstractive approach)

## ğŸ’» Usage

### Installation

```bash
pip install transformers torch
```

### Basic Usage

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Load model and tokenizer
model_name = "NishiKyen/vit5-vietnamese-news"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Input text
text = """
ChÃ­nh phá»§ Viá»‡t Nam Ä‘Ã£ ban hÃ nh quy Ä‘á»‹nh má»›i vá» thuáº¿ thu nháº­p cÃ¡ nhÃ¢n, 
Ã¡p dá»¥ng tá»« ngÃ y 1 thÃ¡ng 1 nÄƒm 2026. Theo Ä‘Ã³, má»©c giáº£m trá»« gia cáº£nh 
sáº½ Ä‘Æ°á»£c tÄƒng tá»« 11 triá»‡u Ä‘á»“ng lÃªn 13 triá»‡u Ä‘á»“ng má»—i thÃ¡ng.
"""

# Tokenize
inputs = tokenizer(
    text,
    max_length=1280,
    truncation=True,
    padding="max_length",
    return_tensors="pt"
)

# Generate summary
outputs = model.generate(
    inputs["input_ids"],
    max_new_tokens=256,
    num_beams=5,
    repetition_penalty=2.5,
    no_repeat_ngram_size=3,
    early_stopping=True
)

# Decode
summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(summary)
```

**Output:**
```
ChÃ­nh phá»§ tÄƒng má»©c giáº£m trá»« gia cáº£nh lÃªn 13 triá»‡u Ä‘á»“ng/thÃ¡ng tá»« 1/1/2026, 
áº£nh hÆ°á»Ÿng Ä‘áº¿n 15 triá»‡u ngÆ°á»i ná»™p thuáº¿ TNCN.
```

### Advanced Usage with Dynamic Length

```python
def summarize_news(text, max_input_length=1280):
    """
    TÃ³m táº¯t tin tá»©c vá»›i Ä‘á»™ dÃ i Ä‘á»™ng
    """
    # Estimate output length based on input
    input_len = len(text.split())
    
    if input_len <= 500:
        max_new = 180
    elif input_len <= 1000:
        max_new = 250
    else:
        max_new = 256
    
    # Tokenize
    inputs = tokenizer(
        text,
        max_length=max_input_length,
        truncation=True,
        return_tensors="pt"
    )
    
    # Generate with optimal parameters
    outputs = model.generate(
        inputs["input_ids"],
        max_new_tokens=max_new,
        min_new_tokens=50,
        num_beams=5,
        length_penalty=1.0,
        repetition_penalty=2.5,
        no_repeat_ngram_size=3,
        early_stopping=True
    )
    
    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return summary

# Usage
long_article = "..."  # Your news article
summary = summarize_news(long_article)
```

## ğŸ—ï¸ Training Details

### Hyperparameters

- **Epochs**: 3
- **Batch Size**: 8 (effective: 8 with gradient accumulation)
- **Learning Rate**: 5e-5
- **Max Input Length**: 1280 tokens
- **Max Output Length**: 256 tokens
- **Optimizer**: AdamW
- **Scheduler**: Linear warmup
- **FP16**: Enabled
- **Repetition Penalty**: 2.5
- **No Repeat N-gram Size**: 3

### Training Configuration

```python
training_args = {
    "output_dir": "./models/vit5_abstractive",
    "num_train_epochs": 3,
    "per_device_train_batch_size": 8,
    "learning_rate": 5e-5,
    "warmup_steps": 500,
    "weight_decay": 0.01,
    "fp16": True,
    "evaluation_strategy": "epoch",
    "save_strategy": "epoch",
    "load_best_model_at_end": True,
}
```

## ğŸ“ Model Architecture

```
ViT5-base (220M parameters)
â”œâ”€â”€ Encoder: 12 layers, 768 hidden, 12 heads
â”œâ”€â”€ Decoder: 12 layers, 768 hidden, 12 heads
â””â”€â”€ Vocabulary: 32,000 SentencePiece tokens
```

## ğŸ“ Citation

```bibtex
@misc{vit5-vietnamese-news,
  author = {Nguyen Trung Kien},
  title = {ViT5 Vietnamese News Summarization},
  year = {2025},
  publisher = {HuggingFace},
  howpublished = {\url{https://huggingface.co/NishiKyen/vit5-vietnamese-news}}
}
```

## ğŸ“ Notes

- MÃ´ hÃ¬nh Ä‘Æ°á»£c fine-tune trÃªn tÃ³m táº¯t **abstractive** (paraphrase), khÃ¡c vá»›i extractive (chá»n cÃ¢u gá»‘c)
- PhÃ¹ há»£p cho tin tá»©c tiáº¿ng Viá»‡t (chÃ­nh trá»‹, kinh táº¿, xÃ£ há»™i, v.v.)
- Output ngáº¯n gá»n hÆ¡n vÃ  tá»± nhiÃªn hÆ¡n so vá»›i extractive model
- Inference time nhanh gáº¥p 2 láº§n so vá»›i extractive variant

## ğŸ”— Related Resources

- **GitHub Repository**: [vietnamese-news-summarization](https://github.com/NishiKyen/vietnamese-news-summarization)
- **Base Model**: [VietAI/vit5-base](https://huggingface.co/VietAI/vit5-base)
- **Dataset**: [8Opt/vietnamese-summarization-dataset](https://huggingface.co/datasets/8Opt/vietnamese-summarization-dataset-0001)
- **Extractive Variant**: *Coming soon*

## ğŸ“§ Contact

- GitHub: [@NishiKyen](https://github.com/NishiKyen)
- Email: nguyentrungkine08102004@gmail.com

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) file for details
